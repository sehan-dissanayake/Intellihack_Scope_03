{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Implementation - Intellihack Scope 03\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system that combines our fine-tuned model with a document retrieval system to provide more accurate and up-to-date answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import RAG components\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.schema.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Import Hugging Face components\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "\n",
    "# For visualizing embeddings (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Set paths\n",
    "data_dir = Path('../data/raw')\n",
    "models_dir = Path('../models')\n",
    "rag_dir = Path('../models/rag')\n",
    "rag_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing for RAG\n",
    "\n",
    "We'll process the documents for our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents for RAG\n",
    "def get_documents():\n",
    "    \"\"\"Load and prepare documents for RAG\"\"\"\n",
    "    # Process raw text files\n",
    "    md_files = list(data_dir.glob('**/*.md'))\n",
    "    txt_files = list(data_dir.glob('**/*.txt'))\n",
    "    all_files = md_files + txt_files\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for file_path in tqdm(all_files, desc=\"Loading documents\"):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            metadata = {\n",
    "                \"source\": str(file_path),\n",
    "                \"filename\": file_path.name,\n",
    "                \"topic\": file_path.stem\n",
    "            }\n",
    "            document = Document(page_content=content, metadata=metadata)\n",
    "            documents.append(document)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load documents\n",
    "documents = get_documents()\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into smaller chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\", \":\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Process documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Efficient model for embedding\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the FAISS vector store\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "vector_store.save_local(rag_dir / \"faiss_index\")\n",
    "\n",
    "print(f\"Vector store created with {len(chunks)} chunks and saved to {rag_dir / 'faiss_index'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fine-tuned Model for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "model_path = models_dir / \"qwen-3b-ai-research-qa\"\n",
    "base_model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# Check if model exists\n",
    "if model_path.exists():\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "    \n",
    "    # Load base model and apply fine-tuned weights\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    \n",
    "    print(\"Fine-tuned model loaded successfully\")\n",
    "else:\n",
    "    print(f\"Fine-tuned model not found at {model_path}. Using base model directly.\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hugging Face pipeline for text generation\n",
    "generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=512,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    do_sample=True,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Create a LangChain HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Retrieve top 3 most similar documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RAG Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG prompt template\n",
    "template = \"\"\"\n",
    "You are an AI assistant specialized in answering questions about AI research, distributed systems, and file systems.\n",
    "You have been specifically trained on topics like the DeepSeek-V3 model, the Fire-Flyer File System (3FS), and AI optimization techniques.\n",
    "\n",
    "Use the following pieces of retrieved context to answer the question. If you don't know the answer based on the context, say that you don't know.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create prompt from template\n",
    "rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Format the retrieved documents function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is DualPipe and how does it improve training efficiency?\",\n",
    "    \"Explain the architecture of Fire-Flyer File System (3FS).\",\n",
    "    \"How does the DeepSeek-V3 model use Mixture-of-Experts architecture?\",\n",
    "    \"What are the advantages of the Chain Replication with Apportioned Queries (CRAQ) in 3FS?\"\n",
    "]\n",
    "\n",
    "# Test the RAG system\n",
    "for question in test_questions:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"Thinking...\")\n",
    "    response = rag_chain.invoke(question)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"=\"*80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare RAG with Direct Model Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate direct model responses\n",
    "def generate_direct_response(question):\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs=inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Compare RAG vs. Direct for a sample question\n",
    "sample_question = \"How does Expert Parallelism Load Balancing work in DeepSeek-V3?\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(\"=\"*80)\n",
    "print(\"Direct Model Response:\")\n",
    "direct_response = generate_direct_response(sample_question)\n",
    "print(direct_response)\n",
    "print(\"=\"*80)\n",
    "print(\"RAG System Response:\")\n",
    "rag_response = rag_chain.invoke(sample_question)\n",
    "print(rag_response)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Retrieved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to inspect retrieved documents\n",
    "def inspect_retrieved_docs(query, top_k=3):\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents for query: '{query}'\")\n",
    "    for i, doc in enumerate(retrieved_docs[:top_k]):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"Content preview (first 300 chars):\")\n",
    "        print(\"-\"*80)\n",
    "        print(doc.page_content[:300], \"...\")\n",
    "        print(\"-\"*80)\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n",
    "# Test with a specific query\n",
    "test_query = \"Explain how DualPipe reduces pipeline bubbles during training\"\n",
    "retrieved_docs = inspect_retrieved_docs(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Document Embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset of documents for visualization\n",
    "max_docs = 100  # Limit to prevent cluttering the visualization\n",
    "sample_chunks = chunks[:min(max_docs, len(chunks))]\n",
    "\n",
    "# Get embeddings for the documents\n",
    "sample_texts = [doc.page_content for doc in sample_chunks]\n",
    "sample_embeddings = embeddings.embed_documents(sample_texts)\n",
    "\n",
    "# Extract topics for coloring\n",
    "topics = [doc.metadata.get('topic', 'unknown') for doc in sample_chunks]\n",
    "unique_topics = list(set(topics))\n",
    "topic_to_color = {topic: i for i, topic in enumerate(unique_topics)}\n",
    "colors = [topic_to_color[topic] for topic in topics]\n",
    "\n",
    "# Use t-SNE to reduce dimensionality for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(sample_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors, alpha=0.6, s=100)\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=plt.cm.tab10(topic_to_color[topic]), \n",
    "                             markersize=10, label=topic) \n",
    "                  for topic in unique_topics]\n",
    "plt.legend(handles=legend_elements, title=\"Topics\")\n",
    "\n",
    "plt.title(\"Document Embeddings Visualization\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the RAG System Components for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the RAG template\n",
    "with open(rag_dir / \"rag_template.txt\", \"w\") as f:\n",
    "    f.write(template)\n",
    "\n",
    "# Save a simple config file for the RAG system\n",
    "rag_config = {\n",
    "    \"embedding_model\": embedding_model_name,\n",
    "    \"vector_store_path\": str(rag_dir / \"faiss_index\"),\n",
    "    \"llm_model_path\": str(model_path),\n",
    "    \"base_model_id\": base_model_id,\n",
    "    \"retriever_k\": 3,\n",
    "    \"generation_params\": {\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.1\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(rag_dir / \"rag_config.json\", \"w\") as f:\n",
    "    json.dump(rag_config, f, indent=2)\n",
    "\n",
    "print(f\"RAG system components saved to {rag_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Simple RAG Demo Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_demo(question):\n",
    "    \"\"\"\n",
    "    Demo function for the RAG system that shows the retrieved context and the generated answer.\n",
    "    \"\"\"\n",
    "    # Get retrieved documents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # Format the prompt\n",
    "    formatted_prompt = rag_prompt.format(context=context, question=question)\n",
    "    \n",
    "    # Generate the answer\n",
    "    response = llm(formatted_prompt)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Retrieved Context:\")\n",
    "    print(\"-\"*80)\n",
    "    print(context[:500], \"...\\n(truncated)\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"Generated Answer:\")\n",
    "    print(\"-\"*80)\n",
    "    print(response)\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the demo function\n",
    "demo_question = \"What are the key innovations in DeepSeek-V3 that made it more efficient to train?\"\n",
    "rag_demo(demo_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented a complete RAG system that combines our fine-tuned model with document retrieval to provide accurate answers to technical questions. This approach enhances the model's capabilities by grounding its responses in the specific technical documents provided.\n",
    "\n",
    "The RAG system includes:\n",
    "1. Document processing and chunking\n",
    "2. Vector embeddings and similarity search\n",
    "3. Integration with our fine-tuned model\n",
    "4. A customized prompt template for high-quality responses\n",
    "\n",
    "This implementation can be further extended with a web interface or API for practical deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
