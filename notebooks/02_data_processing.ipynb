{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing - Intellihack Scope 03\n",
    "\n",
    "This notebook focuses on processing the technical documentation:\n",
    "- Chunking documents into manageable segments\n",
    "- Cleaning and formatting text\n",
    "- Preparing for QA pair generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Chunking Strategies\n",
    "\n",
    "We'll implement different chunking strategies:\n",
    "1. Fixed size chunks (with overlap)\n",
    "2. Semantic chunking (by section/heading)\n",
    "3. Hybrid approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def clean_markdown(text):\n",
    "    \"\"\"\n",
    "    Clean markdown formatting while preserving important content.\n",
    "    \"\"\"\n",
    "    # Remove code blocks but keep their content\n",
    "    text = re.sub(r'```.*?\\n', '', text)\n",
    "    text = re.sub(r'```', '', text)\n",
    "    \n",
    "    # Keep content of links but remove markdown formatting\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
    "    \n",
    "    # Remove other markdown artifacts\n",
    "    text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)  # Bold\n",
    "    text = re.sub(r'\\*([^*]+)\\*', r'\\1', text)        # Italic\n",
    "    \n",
    "    # Clean extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def chunk_by_fixed_size(text, chunk_size=500, overlap=100):\n",
    "    \"\"\"\n",
    "    Split text into fixed-size chunks with overlap.\n",
    "    \"\"\"\n",
    "    # Clean the text first\n",
    "    text = clean_markdown(text)\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_words = len(sentence.split())\n",
    "        \n",
    "        if current_size + sentence_words > chunk_size and current_chunk:\n",
    "            # Save current chunk\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            \n",
    "            # Keep overlap for context continuation\n",
    "            overlap_words = 0\n",
    "            overlap_chunk = []\n",
    "            \n",
    "            for s in reversed(current_chunk):\n",
    "                s_words = len(s.split())\n",
    "                if overlap_words + s_words <= overlap:\n",
    "                    overlap_chunk.insert(0, s)\n",
    "                    overlap_words += s_words\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Start new chunk with overlap\n",
    "            current_chunk = overlap_chunk\n",
    "            current_size = overlap_words\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_size += sentence_words\n",
    "    \n",
    "    # Add the last chunk if not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def chunk_by_section(text):\n",
    "    \"\"\"\n",
    "    Split text by markdown headings to maintain semantic coherence.\n",
    "    \"\"\"\n",
    "    # Find all headings\n",
    "    heading_pattern = re.compile(r'^#{1,6}\\s+.*$', re.MULTILINE)\n",
    "    heading_positions = [(m.start(), m.end()) for m in heading_pattern.finditer(text)]\n",
    "    \n",
    "    if not heading_positions:\n",
    "        # If no headings found, return the entire text as one chunk\n",
    "        return [clean_markdown(text)]\n",
    "    \n",
    "    chunks = []\n",
    "    for i, (start, end) in enumerate(heading_positions):\n",
    "        # Get section heading\n",
    "        heading = text[start:end]\n",
    "        \n",
    "        # Get section content\n",
    "        if i < len(heading_positions) - 1:\n",
    "            section_content = text[end:heading_positions[i+1][0]]\n",
    "        else:\n",
    "            section_content = text[end:]\n",
    "        \n",
    "        # Combine heading and content, clean, and add to chunks\n",
    "        combined = heading + \"\\n\" + section_content\n",
    "        cleaned = clean_markdown(combined)\n",
    "        \n",
    "        # Only add non-empty chunks\n",
    "        if cleaned.strip():\n",
    "            chunks.append(cleaned)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define paths\n",
    "data_dir = Path('../data/raw')\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# List all files\n",
    "files = list(data_dir.glob('**/*.md')) + list(data_dir.glob('**/*.txt'))\n",
    "\n",
    "# Process each file\n",
    "all_chunks = []\n",
    "\n",
    "for file in tqdm(files, desc=\"Processing files\"):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # Get file topic from filename or path\n",
    "    topic = file.stem\n",
    "    \n",
    "    # Try both chunking methods\n",
    "    fixed_chunks = chunk_by_fixed_size(content)\n",
    "    section_chunks = chunk_by_section(content)\n",
    "    \n",
    "    # Choose which chunking method to use based on result quality\n",
    "    # For this implementation, we'll prefer section-based if it produced enough chunks\n",
    "    if len(section_chunks) >= 3:\n",
    "        chunks = section_chunks\n",
    "        chunking_method = \"section\"\n",
    "    else:\n",
    "        chunks = fixed_chunks\n",
    "        chunking_method = \"fixed\"\n",
    "    \n",
    "    # Store metadata with each chunk\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"chunk_id\": f\"{topic}_{i}\",\n",
    "            \"source\": str(file),\n",
    "            \"topic\": topic,\n",
    "            \"chunking_method\": chunking_method,\n",
    "            \"content\": chunk,\n",
    "            \"word_count\": len(chunk.split())\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "chunks_df = pd.DataFrame(all_chunks)\n",
    "chunks_df.to_csv(output_dir / \"chunks.csv\", index=False)\n",
    "\n",
    "# Display statistics\n",
    "print(f\"Total chunks created: {len(chunks_df)}\")\n",
    "print(f\"Average chunk word count: {chunks_df['word_count'].mean():.1f}\")\n",
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the processed chunks for the next stage\n",
    "\n",
    "The processed chunks will be used for QA pair generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save as JSON for easier processing in the next step\n",
    "chunks_json = chunks_df.to_dict(orient='records')\n",
    "with open(output_dir / \"chunks.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_json, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(chunks_json)} processed chunks to {output_dir / 'chunks.json'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}