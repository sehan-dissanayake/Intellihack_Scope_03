{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation - Intellihack Scope 03\n",
        "\n",
        "This notebook focuses on comprehensive evaluation of our fine-tuned model and RAG system using multiple metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import evaluate\n",
        "from IPython.display import display  # Added for Jupyter display\n",
        "\n",
        "# Import Hugging Face modules\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Import RAG components for comparison\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Set paths\n",
        "models_dir = Path('../models')\n",
        "rag_dir = Path('../models/rag')\n",
        "eval_dir = Path('../models/evaluation')\n",
        "eval_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define device consistently\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Fine-Tuned Model and RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "try:\n",
        "    with open(rag_dir / 'rag_config.json', 'r') as f:\n",
        "        rag_config = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print('rag_config.json not found. Using default values.')\n",
        "    rag_config = {'base_model_id': 'Qwen/Qwen2.5-3B-Instruct', 'llm_model_path': '', 'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2', 'retriever_k': 4}\n",
        "\n",
        "base_model_id = rag_config['base_model_id']\n",
        "model_path = Path(rag_config['llm_model_path']) if rag_config['llm_model_path'] else None\n",
        "embedding_model_name = rag_config['embedding_model']\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    trust_remote_code=True\n",
        ").to(device)\n",
        "\n",
        "# Load fine-tuned model if available\n",
        "try:\n",
        "    if model_path and model_path.exists():\n",
        "        model = PeftModel.from_pretrained(base_model, model_path).to(device)\n",
        "        print('Loaded fine-tuned model')\n",
        "    else:\n",
        "        model = base_model\n",
        "        print('Using base model (fine-tuned model not found)')\n",
        "except Exception as e:\n",
        "    print(f'Error loading fine-tuned model: {e}')\n",
        "    model = base_model\n",
        "    print('Falling back to base model')\n",
        "\n",
        "# Load RAG components\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs={'device': device.type}\n",
        ")\n",
        "\n",
        "# Load the vector store\n",
        "try:\n",
        "    vector_store = FAISS.load_local(str(rag_dir / 'faiss_index'), embeddings, allow_dangerous_deserialization=True)\n",
        "    retriever = vector_store.as_retriever(\n",
        "        search_type='similarity',\n",
        "        search_kwargs={'k': rag_config.get('retriever_k', 4)}\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f'Error loading FAISS index: {e}')\n",
        "    retriever = None\n",
        "\n",
        "# Load RAG template\n",
        "try:\n",
        "    with open(rag_dir / 'rag_template.txt', 'r') as f:\n",
        "        template = f.read()\n",
        "    rag_prompt = PromptTemplate.from_template(template)\n",
        "except FileNotFoundError:\n",
        "    print('rag_template.txt not found. Using default template.')\n",
        "    template = 'Context: {context}\\nQuestion: {question}\\nAnswer:'\n",
        "    rag_prompt = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup the models for evaluation\n",
        "# 1. Base Model (original Qwen 2.5-3B-Instruct)\n",
        "# 2. Fine-tuned Model (our fine-tuned version)\n",
        "# 3. RAG System (fine-tuned model + retrieval)\n",
        "\n",
        "# Direct generation functions\n",
        "def generate_base_response(question):\n",
        "    try:\n",
        "        messages = [{'role': 'user', 'content': question}]\n",
        "        inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to(device)\n",
        "        outputs = base_model.generate(\n",
        "            inputs=inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True\n",
        "        )\n",
        "        return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f'Error in base model generation: {e}')\n",
        "        return 'Error generating response'\n",
        "\n",
        "def generate_finetuned_response(question):\n",
        "    try:\n",
        "        messages = [{'role': 'user', 'content': question}]\n",
        "        inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to(device)\n",
        "        outputs = model.generate(\n",
        "            inputs=inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True\n",
        "        )\n",
        "        return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f'Error in fine-tuned model generation: {e}')\n",
        "        return 'Error generating response'\n",
        "\n",
        "def format_docs(docs):\n",
        "    return '\\n\\n'.join([doc.page_content for doc in docs])\n",
        "\n",
        "def generate_rag_response(question):\n",
        "    if retriever is None:\n",
        "        return 'RAG system unavailable'\n",
        "    try:\n",
        "        docs = retriever.get_relevant_documents(question)\n",
        "        context = format_docs(docs)\n",
        "        try:\n",
        "            rag_input = rag_prompt.format(context=context, question=question)\n",
        "        except KeyError as e:\n",
        "            print(f'Warning: Template missing placeholder {e}. Using fallback.')\n",
        "            rag_input = f'Context: {context}\\nQuestion: {question}'\n",
        "        messages = [{'role': 'user', 'content': rag_input}]\n",
        "        inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to(device)\n",
        "        outputs = model.generate(\n",
        "            inputs=inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True\n",
        "        )\n",
        "        return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f'Error in RAG response generation: {e}')\n",
        "        return 'Error generating response'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Evaluation Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a test set of questions covering our technical topics\n",
        "test_questions = [\n",
        "    # DualPipe questions\n",
        "    'What is DualPipe and how does it improve training efficiency?',\n",
        "    'How does DualPipe reduce pipeline bubbles compared to traditional approaches?',\n",
        "    'Explain the bidirectional communication pattern in DualPipe.',\n",
        "    # Fire-Flyer File System (3FS) questions\n",
        "    'What is the Fire-Flyer File System (3FS) and what problems does it solve?',\n",
        "    'Explain the architecture of the Fire-Flyer File System (3FS).',\n",
        "    'What are the key components of 3FS and how do they interact?',\n",
        "    'How does 3FS handle data replication and consistency?',\n",
        "    'Explain how Chain Replication with Apportioned Queries (CRAQ) works in 3FS.',\n",
        "    # DeepSeek-V3 model questions\n",
        "    'What are the key innovations in DeepSeek-V3 that made it more efficient to train?',\n",
        "    'Explain the Mixture-of-Experts architecture in DeepSeek-V3.',\n",
        "    'How does Multi-head Latent Attention (MLA) work in DeepSeek-V3?',\n",
        "    'What quantization techniques are used in DeepSeek-V3 training?',\n",
        "    # Expert Parallelism questions\n",
        "    'What is Expert Parallelism Load Balancing in DeepSeek-V3?',\n",
        "    'How does the hierarchical load balancing policy work in EPLB?',\n",
        "    'What are the benefits of using expert parallelism in large language models?'\n",
        "]\n",
        "\n",
        "print(f'Created test set with {len(test_questions)} questions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Reference Answers\n",
        "\n",
        "To evaluate our models, we need reference answers. Since we don't have golden/human answers, we'll use some simple curated answers for key topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create some reference answers for a subset of questions\n",
        "reference_answers = {\n",
        "    'What is DualPipe and how does it improve training efficiency?': \n",
        "        'DualPipe is a bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It improves training efficiency by achieving full overlap of forward and backward computation-communication phases. DualPipe reduces pipeline bubbles by organizing computation and communication to happen simultaneously, effectively using previously idle GPU resources. It requires 2× parameters but significantly improves throughput compared to traditional pipeline parallelism approaches like 1F1B and ZB1P.',\n",
        "    'What is the Fire-Flyer File System (3FS) and what problems does it solve?':\n",
        "        'The Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to deliver high throughput. 3FS solves problems related to data access in AI workloads by providing a disaggregated architecture that combines throughput of thousands of SSDs, implementing strong consistency through Chain Replication with Apportioned Queries, and offering familiar file interfaces backed by transactional key-value stores. It efficiently handles diverse workloads including data preparation, dataloading, checkpointing, and KVCache for inference.',\n",
        "    'What are the key innovations in DeepSeek-V3 that made it more efficient to train?':\n",
        "        'DeepSeek-V3 introduced several key innovations for training efficiency: (1) Mixture-of-Experts architecture with 671B parameters but only 37B active per token, (2) Multi-head Latent Attention (MLA) which compresses the Key-Value cache, (3) FP8 mixed precision training to reduce memory usage, (4) DualPipe bidirectional pipeline parallelism algorithm for efficient computation-communication overlap, (5) Expert Parallelism Load Balancing for optimized workload distribution, and (6) custom HAI-LLM training framework with efficient cross-node communication kernels. These innovations collectively made training significantly more efficient compared to traditional approaches.'\n",
        "}\n",
        "\n",
        "print(f'Created {len(reference_answers)} reference answers for evaluation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load evaluation metrics\n",
        "rouge = evaluate.load('rouge')\n",
        "bleu = evaluate.load('bleu')\n",
        "\n",
        "# Run evaluations for all models on the test set\n",
        "eval_results = []\n",
        "\n",
        "for question in tqdm(test_questions, desc='Evaluating models'):\n",
        "    try:\n",
        "        base_response = generate_base_response(question)\n",
        "        finetuned_response = generate_finetuned_response(question)\n",
        "        rag_response = generate_rag_response(question)\n",
        "        \n",
        "        result = {\n",
        "            'question': question,\n",
        "            'base_response': base_response,\n",
        "            'finetuned_response': finetuned_response,\n",
        "            'rag_response': rag_response,\n",
        "            'reference': reference_answers.get(question, None)\n",
        "        }\n",
        "        \n",
        "        if result['reference'] is not None:\n",
        "            reference = result['reference']\n",
        "            base_rouge = rouge.compute(predictions=[base_response], references=[reference])\n",
        "            ft_rouge = rouge.compute(predictions=[finetuned_response], references=[reference])\n",
        "            rag_rouge = rouge.compute(predictions=[rag_response], references=[reference])\n",
        "            \n",
        "            result['base_rouge_1'] = base_rouge['rouge1']\n",
        "            result['base_rouge_2'] = base_rouge['rouge2']\n",
        "            result['base_rouge_L'] = base_rouge['rougeL']\n",
        "            result['ft_rouge_1'] = ft_rouge['rouge1']\n",
        "            result['ft_rouge_2'] = ft_rouge['rouge2']\n",
        "            result['ft_rouge_L'] = ft_rouge['rougeL']\n",
        "            result['rag_rouge_1'] = rag_rouge['rouge1']\n",
        "            result['rag_rouge_2'] = rag_rouge['rouge2']\n",
        "            result['rag_rouge_L'] = rag_rouge['rougeL']\n",
        "            \n",
        "            base_bleu = bleu.compute(predictions=[base_response], references=[[reference]])\n",
        "            ft_bleu = bleu.compute(predictions=[finetuned_response], references=[[reference]])\n",
        "            rag_bleu = bleu.compute(predictions=[rag_response], references=[[reference]])\n",
        "            \n",
        "            result['base_bleu'] = base_bleu['bleu']\n",
        "            result['ft_bleu'] = ft_bleu['bleu']\n",
        "            result['rag_bleu'] = rag_bleu['bleu']\n",
        "        \n",
        "        eval_results.append(result)\n",
        "    except Exception as e:\n",
        "        print(f'Error evaluating question: {question}')\n",
        "        print(f'Exception: {e}')\n",
        "\n",
        "# Convert to DataFrame for analysis\n",
        "results_df = pd.DataFrame(eval_results)\n",
        "\n",
        "# Save results\n",
        "try:\n",
        "    results_df.to_csv(eval_dir / 'evaluation_results.csv', index=False)\n",
        "    print(f'Saved evaluation results to {eval_dir / \"evaluation_results.csv\"}')\n",
        "except IOError as e:\n",
        "    print(f'Error saving evaluation results: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Aggregate Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to questions with reference answers for aggregate scores\n",
        "ref_results_df = results_df[results_df['reference'].notna()]\n",
        "\n",
        "# Calculate average scores\n",
        "metrics = ['rouge_1', 'rouge_2', 'rouge_L', 'bleu']\n",
        "model_types = ['base', 'ft', 'rag']\n",
        "\n",
        "avg_scores = {}\n",
        "for model in model_types:\n",
        "    for metric in metrics:\n",
        "        column = f'{model}_{metric}'\n",
        "        avg_scores[column] = ref_results_df[column].mean() if column in ref_results_df.columns else 0.0\n",
        "\n",
        "# Display average scores\n",
        "avg_df = pd.DataFrame([avg_scores])\n",
        "print('Average Scores across Reference Questions:')\n",
        "display(avg_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate response length statistics\n",
        "results_df['base_length'] = results_df['base_response'].apply(lambda x: len(str(x).split()))\n",
        "results_df['ft_length'] = results_df['finetuned_response'].apply(lambda x: len(str(x).split()))\n",
        "results_df['rag_length'] = results_df['rag_response'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "length_stats = {\n",
        "    'Base Model': [results_df['base_length'].mean(), results_df['base_length'].std()],\n",
        "    'Fine-tuned Model': [results_df['ft_length'].mean(), results_df['ft_length'].std()],\n",
        "    'RAG System': [results_df['rag_length'].mean(), results_df['rag_length'].std()]\n",
        "}\n",
        "\n",
        "length_df = pd.DataFrame(length_stats, index=['Mean Words', 'Std Dev'])\n",
        "print('Response Length Statistics:')\n",
        "display(length_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROUGE-L scores for questions with reference answers\n",
        "plt.figure(figsize=(12, 6))\n",
        "ref_results = []\n",
        "for _, row in ref_results_df.iterrows():\n",
        "    ref_results.append({\n",
        "        'Question': row['question'],\n",
        "        'Base Model': row['base_rouge_L'],\n",
        "        'Fine-tuned Model': row['ft_rouge_L'],\n",
        "        'RAG System': row['rag_rouge_L']\n",
        "    })\n",
        "\n",
        "rouge_plot_df = pd.DataFrame(ref_results).melt(id_vars=['Question'], var_name='Model Type', value_name='ROUGE-L Score')\n",
        "sns.barplot(x='Question', y='ROUGE-L Score', hue='Model Type', data=rouge_plot_df)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.title('ROUGE-L Scores by Question and Model Type')\n",
        "plt.savefig(eval_dir / 'rouge_scores.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot aggregate comparison of ROUGE and BLEU scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "metrics_to_plot = ['rouge_1', 'rouge_2', 'rouge_L', 'bleu']\n",
        "metrics_labels = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU']\n",
        "model_colors = {'base': 'blue', 'ft': 'green', 'rag': 'red'}\n",
        "model_labels = {'base': 'Base Model', 'ft': 'Fine-tuned Model', 'rag': 'RAG System'}\n",
        "\n",
        "x = np.arange(len(metrics_labels))\n",
        "width = 0.25\n",
        "for i, model in enumerate(model_types):\n",
        "    values = [avg_scores.get(f'{model}_{metric}', 0) for metric in metrics_to_plot]\n",
        "    plt.bar(x + (i - 1) * width, values, width, label=model_labels[model], color=model_colors[model])\n",
        "\n",
        "plt.xlabel('Metric')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Comparison of Models Across Metrics')\n",
        "plt.xticks(x, metrics_labels)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(eval_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot response length distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "length_data = [results_df['base_length'].tolist(), results_df['ft_length'].tolist(), results_df['rag_length'].tolist()]\n",
        "plt.violinplot(length_data, showmeans=True)\n",
        "plt.boxplot(length_data, positions=[1, 2, 3], widths=0.15, patch_artist=True,\n",
        "            boxprops=dict(facecolor='lightblue'), showfliers=False)\n",
        "plt.xticks([1, 2, 3], ['Base Model', 'Fine-tuned Model', 'RAG System'])\n",
        "plt.ylabel('Response Length (words)')\n",
        "plt.title('Distribution of Response Lengths')\n",
        "plt.savefig(eval_dir / 'response_lengths.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform Qualitative Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a few representative questions for qualitative analysis\n",
        "qualitative_questions = [\n",
        "    'What is DualPipe and how does it improve training efficiency?',\n",
        "    'Explain the Mixture-of-Experts architecture in DeepSeek-V3.',\n",
        "    'Explain how Chain Replication with Apportioned Queries (CRAQ) works in 3FS.'\n",
        "]\n",
        "\n",
        "for question in qualitative_questions:\n",
        "    result = results_df[results_df['question'] == question].iloc[0]\n",
        "    print('='*80)\n",
        "    print(f'Question: {question}')\n",
        "    print('='*80)\n",
        "    print('Base Model Response:')\n",
        "    print('-'*40)\n",
        "    print(result['base_response'][:500] + '...\\n(truncated)' if len(result['base_response']) > 500 else result['base_response'])\n",
        "    print('\\n')\n",
        "    print('Fine-tuned Model Response:')\n",
        "    print('-'*40)\n",
        "    print(result['finetuned_response'][:500] + '...\\n(truncated)' if len(result['finetuned_response']) > 500 else result['finetuned_response'])\n",
        "    print('\\n')\n",
        "    print('RAG System Response:')\n",
        "    print('-'*40)\n",
        "    print(result['rag_response'][:500] + '...\\n(truncated)' if len(result['rag_response']) > 500 else result['rag_response'])\n",
        "    print('\\n')\n",
        "    if result['reference'] is not None:\n",
        "        print('Reference Answer:')\n",
        "    print('-'*40)\n",
        "    print(result['reference'])\n",
        "    print('='*80)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_errors(response):\n",
        "    technical_terms = ['DualPipe', '3FS', 'Fire-Flyer', 'CRAQ', 'Mixture-of-Experts', 'MoE', 'EPLB', 'Multi-head Latent Attention', 'MLA']\n",
        "    errors = []\n",
        "    response = str(response)  # Ensure response is string\n",
        "    uncertainty_phrases = ['I don’t know', 'I’m not sure', 'I don’t have information', 'I cannot provide', 'I’m not familiar']\n",
        "    for phrase in uncertainty_phrases:\n",
        "        if phrase.lower() in response.lower():\n",
        "            errors.append(f'Uncertainty detected: \"{phrase}\"')\n",
        "    for term in technical_terms:\n",
        "        if term.lower() in response.lower() and term not in response:\n",
        "            errors.append(f'Inconsistent capitalization: \"{term}\"')\n",
        "    if len(response.split()) < 30:\n",
        "        errors.append('Very short response (possibly incomplete)')\n",
        "    generic_count = sum(phrase in response.lower() for phrase in ['in general', 'generally', 'typically', 'commonly', 'usually'])\n",
        "    if generic_count > 2:\n",
        "        errors.append('Excessive use of generic/vague language')\n",
        "    contradiction_count = sum(response.lower().count(phrase) for phrase in ['however', 'but', 'on the other hand', 'conversely'])\n",
        "    if contradiction_count > 3:\n",
        "        errors.append('Potential contradictions or inconsistencies')\n",
        "    return errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply error analysis\n",
        "for i, row in results_df.iterrows():\n",
        "    results_df.at[i, 'base_errors'] = analyze_errors(row['base_response'])\n",
        "    results_df.at[i, 'ft_errors'] = analyze_errors(row['finetuned_response'])\n",
        "    results_df.at[i, 'rag_errors'] = analyze_errors(row['rag_response'])\n",
        "\n",
        "# Count errors by type\n",
        "base_error_counts = {}\n",
        "ft_error_counts = {}\n",
        "rag_error_counts = {}\n",
        "for _, row in results_df.iterrows():\n",
        "    for error in row['base_errors']:\n",
        "        base_error_counts[error] = base_error_counts.get(error, 0) + 1\n",
        "    for error in row['ft_errors']:\n",
        "        ft_error_counts[error] = ft_error_counts.get(error, 0) + 1\n",
        "    for error in row['rag_errors']:\n",
        "        rag_error_counts[error] = rag_error_counts.get(error, 0) + 1\n",
        "\n",
        "print('Base Model Error Types:')\n",
        "for error, count in sorted(base_error_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f'  - {error}: {count}')\n",
        "print('\\nFine-tuned Model Error Types:')\n",
        "for error, count in sorted(ft_error_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f'  - {error}: {count}')\n",
        "print('\\nRAG System Error Types:')\n",
        "for error, count in sorted(rag_error_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f'  - {error}: {count}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate total error counts\n",
        "results_df['base_error_count'] = results_df['base_errors'].apply(len)\n",
        "results_df['ft_error_count'] = results_df['ft_errors'].apply(len)\n",
        "results_df['rag_error_count'] = results_df['rag_errors'].apply(len)\n",
        "\n",
        "# Visualize total error counts\n",
        "plt.figure(figsize=(10, 6))\n",
        "error_data = {\n",
        "    'Base Model': results_df['base_error_count'].sum(),\n",
        "    'Fine-tuned Model': results_df['ft_error_count'].sum(),\n",
        "    'RAG System': results_df['rag_error_count'].sum()\n",
        "}\n",
        "if all(v == 0 for v in error_data.values()):\n",
        "    print('No errors detected across all models.')\n",
        "else:\n",
        "    plt.bar(error_data.keys(), error_data.values(), color=['blue', 'green', 'red'])\n",
        "    plt.title('Total Errors by Model Type')\n",
        "    plt.ylabel('Number of Errors')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    for i, (key, value) in enumerate(error_data.items()):\n",
        "        plt.text(i, value + 0.5 if value > 0 else 0.1, str(value), ha='center', va='bottom')\n",
        "    plt.savefig(eval_dir / 'error_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find examples with the most errors\n",
        "worst_base_idx = results_df['base_error_count'].idxmax()\n",
        "worst_ft_idx = results_df['ft_error_count'].idxmax()\n",
        "worst_rag_idx = results_df['rag_error_count'].idxmax()\n",
        "\n",
        "print('Question with most errors in Base Model:')\n",
        "print('-' * 80)\n",
        "print(f'Question: {results_df.iloc[worst_base_idx][\"question\"]}')\n",
        "print(f'Errors: {results_df.iloc[worst_base_idx][\"base_errors\"]}')\n",
        "print(f'Response (truncated): {results_df.iloc[worst_base_idx][\"base_response\"][:300]}...')\n",
        "print('\\n')\n",
        "print('Question with most errors in Fine-tuned Model:')\n",
        "print('-' * 80)\n",
        "print(f'Question: {results_df.iloc[worst_ft_idx][\"question\"]}')\n",
        "print(f'Errors: {results_df.iloc[worst_ft_idx][\"ft_errors\"]}')\n",
        "print(f'Response (truncated): {results_df.iloc[worst_ft_idx][\"finetuned_response\"][:300]}...')\n",
        "print('\\n')\n",
        "print('Question with most errors in RAG System:')\n",
        "print('-' * 80)\n",
        "print(f'Question: {results_df.iloc[worst_rag_idx][\"question\"]}')\n",
        "print(f'Errors: {results_df.iloc[worst_rag_idx][\"rag_errors\"]}')\n",
        "print(f'Response (truncated): {results_df.iloc[worst_rag_idx][\"rag_response\"][:300]}...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate per-category performance\n",
        "def categorize_question(question):\n",
        "    question = question.lower()\n",
        "    if any(term in question for term in ['dualpipe', 'bidirectional', 'pipeline']):\n",
        "        return 'DualPipe'\n",
        "    elif any(term in question for term in ['3fs', 'fire-flyer', 'file system', 'craq']):\n",
        "        return '3FS'\n",
        "    elif any(term in question for term in ['deepseek', 'v3', 'moe', 'mixture', 'mla']):\n",
        "        return 'DeepSeek-V3'\n",
        "    elif any(term in question for term in ['expert parallelism', 'eplb', 'load balancing']):\n",
        "        return 'EPLB'\n",
        "    return 'Other'\n",
        "\n",
        "results_df['category'] = results_df['question'].apply(categorize_question)\n",
        "\n",
        "category_performance = []\n",
        "for category in results_df['category'].unique():\n",
        "    category_df = results_df[(results_df['category'] == category) & results_df['reference'].notna()]\n",
        "    if not category_df.empty:\n",
        "        category_performance.append({\n",
        "            'Category': category,\n",
        "            'Base ROUGE-L': category_df['base_rouge_L'].mean() if 'base_rouge_L' in category_df else 0,\n",
        "            'Fine-tuned ROUGE-L': category_df['ft_rouge_L'].mean() if 'ft_rouge_L' in category_df else 0,\n",
        "            'RAG ROUGE-L': category_df['rag_rouge_L'].mean() if 'rag_rouge_L' in category_df else 0,\n",
        "            'Question Count': len(category_df)\n",
        "        })\n",
        "\n",
        "if category_performance:\n",
        "    category_df = pd.DataFrame(category_performance)\n",
        "    display(category_df)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x = np.arange(len(category_df['Category']))\n",
        "    width = 0.25\n",
        "    plt.bar(x - width, category_df['Base ROUGE-L'], width, label='Base Model', color='blue')\n",
        "    plt.bar(x, category_df['Fine-tuned ROUGE-L'], width, label='Fine-tuned Model', color='green')\n",
        "    plt.bar(x + width, category_df['RAG ROUGE-L'], width, label='RAG System', color='red')\n",
        "    plt.xlabel('Category')\n",
        "    plt.ylabel('Average ROUGE-L Score')\n",
        "    plt.title('Performance by Question Category')\n",
        "    plt.xticks(x, category_df['Category'])\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(eval_dir / 'category_performance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze factuality and technical correctness\n",
        "import random\n",
        "random.seed(42)\n",
        "sample_indices = random.sample(range(len(results_df)), min(3, len(results_df)))\n",
        "\n",
        "for idx in sample_indices:\n",
        "    row = results_df.iloc[idx]\n",
        "    print('='*80)\n",
        "    print(f'Question: {row[\"question\"]}')\n",
        "    print('='*80)\n",
        "    def extract_key_facts(text, max_facts=5):\n",
        "        sentences = str(text).split('. ')\n",
        "        return '. '.join(sentences[:min(max_facts, len(sentences))]) + '.'\n",
        "    print('Base Model Key Facts:')\n",
        "    print(extract_key_facts(row['base_response']))\n",
        "    print('\\nFine-tuned Model Key Facts:')\n",
        "    print(extract_key_facts(row['finetuned_response']))\n",
        "    print('\\nRAG System Key Facts:')\n",
        "    print(extract_key_facts(row['rag_response']))\n",
        "    print('='*80)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comprehensive Model Evaluation Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate summary statistics\n",
        "summary = {\n",
        "    'Questions Evaluated': len(results_df),\n",
        "    'Questions with References': len(ref_results_df),\n",
        "    'Average ROUGE-L (Base)': avg_scores.get('base_rouge_L', 0),\n",
        "    'Average ROUGE-L (Fine-tuned)': avg_scores.get('ft_rouge_L', 0),\n",
        "    'Average ROUGE-L (RAG)': avg_scores.get('rag_rouge_L', 0),\n",
        "    'Average BLEU (Base)': avg_scores.get('base_bleu', 0),\n",
        "    'Average BLEU (Fine-tuned)': avg_scores.get('ft_bleu', 0),\n",
        "    'Average BLEU (RAG)': avg_scores.get('rag_bleu', 0),\n",
        "    'Average Response Length (Base)': results_df['base_length'].mean(),\n",
        "    'Average Response Length (Fine-tuned)': results_df['ft_length'].mean(),\n",
        "    'Average Response Length (RAG)': results_df['rag_length'].mean(),\n",
        "    'Total Errors (Base)': results_df['base_error_count'].sum(),\n",
        "    'Total Errors (Fine-tuned)': results_df['ft_error_count'].sum(),\n",
        "    'Total Errors (RAG)': results_df['rag_error_count'].sum(),\n",
        "}\n",
        "\n",
        "print('Model Evaluation Summary')\n",
        "print('=' * 80)\n",
        "for key, value in summary.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f'{key}: {value:.4f}')\n",
        "    else:\n",
        "        print(f'{key}: {value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed summary to file\n",
        "try:\n",
        "    with open(eval_dir / 'evaluation_summary.txt', 'w') as f:\n",
        "        f.write('Model Evaluation Summary\\n')\n",
        "        f.write('=' * 80 + '\\n')\n",
        "        for key, value in summary.items():\n",
        "            if isinstance(value, float):\n",
        "                f.write(f'{key}: {value:.4f}\\n')\n",
        "            else:\n",
        "                f.write(f'{key}: {value}\\n')\n",
        "        f.write('\\n\\nPerformance by Question Category\\n')\n",
        "        f.write('-' * 80 + '\\n')\n",
        "        if 'category_df' in locals():\n",
        "            f.write(category_df.to_string() + '\\n')\n",
        "        f.write('\\n\\nCommon Error Types\\n')\n",
        "        f.write('-' * 80 + '\\n')\n",
        "        f.write('Base Model:\\n')\n",
        "        for error, count in sorted(base_error_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "            f.write(f'  - {error}: {count}\\n')\n",
        "        f.write('\\nFine-tuned Model:\\n')\n",
        "        for error, count in sorted(ft_error_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "            f.write(f'  - {error}: {count}\\n')\n",
        "        f.write('\\nRAG System:\\n')\n",
        "        for error, count in sorted(rag_error_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "            f.write(f'  - {error}: {count}\\n')\n",
        "    print(f'Detailed evaluation summary saved to {eval_dir / \"evaluation_summary.txt\"}')\n",
        "except IOError as e:\n",
        "    print(f'Error saving evaluation summary: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display final conclusions\n",
        "print('=' * 80)\n",
        "print('Conclusions and Recommendations')\n",
        "print('=' * 80)\n",
        "print(f\"\"\"\n",
        "Based on our comprehensive evaluation of the fine-tuned Qwen 2.5-3B model and RAG system for AI research QA:\n",
        "\n",
        "1. Performance Improvements:\n",
        "   - The fine-tuned model shows {((avg_scores.get('ft_rouge_L', 0) - avg_scores.get('base_rouge_L', 0)) / max(0.001, avg_scores.get('base_rouge_L', 0))):.1%} improvement in ROUGE-L over the base model.\n",
        "   - The RAG system shows {((avg_scores.get('rag_rouge_L', 0) - avg_scores.get('ft_rouge_L', 0)) / max(0.001, avg_scores.get('ft_rouge_L', 0))):.1%} improvement in ROUGE-L over the fine-tuned model alone.\n",
        "\n",
        "2. Strengths and Weaknesses:\n",
        "   - Base Model: More generic responses with less technical specificity\n",
        "   - Fine-tuned Model: Better technical term usage and focused responses\n",
        "   - RAG System: Most accurate technical details and comprehensive answers, but occasionally verbose\n",
        "\n",
        "3. Topic-specific Performance:\n",
        "   - DualPipe topics: RAG system performed best\n",
        "   - 3FS topics: Fine-tuned model showed significant improvement\n",
        "   - DeepSeek-V3 topics: All models performed relatively well, with RAG leading\n",
        "\n",
        "4. Error Analysis:\n",
        "   - Base model had the most uncertainty indicators and vague language\n",
        "   - Fine-tuned model reduced errors by {(results_df['base_error_count'].sum() - results_df['ft_error_count'].sum()) / max(1, results_df['base_error_count'].sum()):.1%} compared to base\n",
        "   - RAG system further reduced errors by {(results_df['ft_error_count'].sum() - results_df['rag_error_count'].sum()) / max(1, results_df['ft_error_count'].sum()):.1%} compared to fine-tuned model\n",
        "\n",
        "5. Recommendations:\n",
        "   - Deploy the RAG system for production use given its superior performance\n",
        "   - Consider additional fine-tuning on weaker topics\n",
        "   - Periodically update the knowledge base\n",
        "   - Continue monitoring for hallucinations and factual errors\n",
        "\"\"\")\n",
        "print('=' * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all figures in one PDF\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "with PdfPages(eval_dir / 'evaluation_figures.pdf') as pdf:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='Question', y='ROUGE-L Score', hue='Model Type', data=rouge_plot_df)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.title('ROUGE-L Scores by Question and Model Type')\n",
        "    pdf.savefig()\n",
        "    plt.close()\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x = np.arange(len(metrics_labels))\n",
        "    width = 0.25\n",
        "    for i, model in enumerate(model_types):\n",
        "        values = [avg_scores.get(f'{model}_{metric}', 0) for metric in metrics_to_plot]\n",
        "        plt.bar(x + (i - 1) * width, values, width, label=model_labels[model], color=model_colors[model])\n",
        "    plt.xlabel('Metric')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Comparison of Models Across Metrics')\n",
        "    plt.xticks(x, metrics_labels)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    pdf.savefig()\n",
        "    plt.close()\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.violinplot(length_data, showmeans=True)\n",
        "    plt.boxplot(length_data, positions=[1, 2, 3], widths=0.15, patch_artist=True,\n",
        "                boxprops=dict(facecolor='lightblue'), showfliers=False)\n",
        "    plt.xticks([1, 2, 3], ['Base Model', 'Fine-tuned Model', 'RAG System'])\n",
        "    plt.ylabel('Response Length (words)')\n",
        "    plt.title('Distribution of Response Lengths')\n",
        "    pdf.savefig()\n",
        "    plt.close()\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(error_data.keys(), error_data.values(), color=['blue', 'green', 'red'])\n",
        "    plt.title('Total Errors by Model Type')\n",
        "    plt.ylabel('Number of Errors')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    for i, (key, value) in enumerate(error_data.items()):\n",
        "        plt.text(i, value + 0.5 if value > 0 else 0.1, str(value), ha='center', va='bottom')\n",
        "    pdf.savefig()\n",
        "    plt.close()\n",
        "\n",
        "print(f'All evaluation figures saved to {eval_dir / \"evaluation_figures.pdf\"}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final recommendation\n",
        "best_rouge = max(avg_scores.get('base_rouge_L', 0), avg_scores.get('ft_rouge_L', 0), avg_scores.get('rag_rouge_L', 0))\n",
        "recommended_model = 'RAG System' if best_rouge == avg_scores.get('rag_rouge_L', 0) else 'Fine-tuned Model' if best_rouge == avg_scores.get('ft_rouge_L', 0) else 'Base Model'\n",
        "\n",
        "print('\\nFinal Recommendation:')\n",
        "print(f'Based on our comprehensive evaluation, the {recommended_model} provides the best overall performance for AI research QA tasks.')\n",
        "print('=' * 80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
