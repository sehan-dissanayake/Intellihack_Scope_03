{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation - Intellihack Scope 03\n",
    "\n",
    "This notebook focuses on comprehensive evaluation of our fine-tuned model and RAG system using multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import evaluate\n",
    "\n",
    "# Import Hugging Face modules\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Import RAG components for comparison\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set paths\n",
    "models_dir = Path('../models')\n",
    "rag_dir = Path('../models/rag')\n",
    "eval_dir = Path('../models/evaluation')\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fine-Tuned Model and RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load configuration\n",
    "with open(rag_dir / \"rag_config.json\", \"r\") as f:\n",
    "    rag_config = json.load(f)\n",
    "\n",
    "base_model_id = rag_config[\"base_model_id\"]\n",
    "model_path = Path(rag_config[\"llm_model_path\"])\n",
    "embedding_model_name = rag_config[\"embedding_model\"]\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load fine-tuned model if available\n",
    "if model_path.exists():\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    print(\"Loaded fine-tuned model\")\n",
    "else:\n",
    "    model = base_model\n",
    "    print(\"Using base model (fine-tuned model not found)\")\n",
    "\n",
    "# Load RAG components\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    ")\n",
    "\n",
    "# Load the vector store\n",
    "vector_store = FAISS.load_local(rag_dir / \"faiss_index\", embeddings)\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": rag_config[\"retriever_k\"]}\n",
    ")\n",
    "\n",
    "# Load RAG template\n",
    "with open(rag_dir / \"rag_template.txt\", \"r\") as f:\n",
    "    template = f.read()\n",
    "    \n",
    "rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup the models for evaluation\n",
    "# 1. Base Model (original Qwen 2.5-3B-Instruct)\n",
    "# 2. Fine-tuned Model (our fine-tuned version)\n",
    "# 3. RAG System (fine-tuned model + retrieval)\n",
    "\n",
    "# Create evaluation pipelines\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Direct generation functions\n",
    "def generate_base_response(question):\n",
    "    \"\"\"\n",
    "    Generate response from the base model.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(base_model.device)\n",
    "    \n",
    "    outputs = base_model.generate(\n",
    "        inputs=inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def generate_finetuned_response(question):\n",
    "    \"\"\"\n",
    "    Generate response from the fine-tuned model.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs=inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def generate_rag_response(question):\n",
    "    \"\"\"\n",
    "    Generate response using the RAG system.\n",
    "    \"\"\"\n",
    "    # Get relevant documents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # Format prompt with context\n",
    "    rag_input = rag_prompt.format(context=context, question=question)\n",
    "    \n",
    "    # Generate using fine-tuned model\n",
    "    messages = [{\"role\": \"user\", \"content\": rag_input}]\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs=inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Evaluation Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a test set of questions covering our technical topics\n",
    "test_questions = [\n",
    "    # DualPipe questions\n",
    "    \"What is DualPipe and how does it improve training efficiency?\",\n",
    "    \"How does DualPipe reduce pipeline bubbles compared to traditional approaches?\",\n",
    "    \"Explain the bidirectional communication pattern in DualPipe.\",\n",
    "    \n",
    "    # Fire-Flyer File System (3FS) questions\n",
    "    \"What is the Fire-Flyer File System (3FS) and what problems does it solve?\",\n",
    "    \"Explain the architecture of the Fire-Flyer File System (3FS).\",\n",
    "    \"What are the key components of 3FS and how do they interact?\",\n",
    "    \"How does 3FS handle data replication and consistency?\",\n",
    "    \"Explain how Chain Replication with Apportioned Queries (CRAQ) works in 3FS.\",\n",
    "    \n",
    "    # DeepSeek-V3 model questions\n",
    "    \"What are the key innovations in DeepSeek-V3 that made it more efficient to train?\",\n",
    "    \"Explain the Mixture-of-Experts architecture in DeepSeek-V3.\",\n",
    "    \"How does Multi-head Latent Attention (MLA) work in DeepSeek-V3?\",\n",
    "    \"What quantization techniques are used in DeepSeek-V3 training?\",\n",
    "    \n",
    "    # Expert Parallelism questions\n",
    "    \"What is Expert Parallelism Load Balancing in DeepSeek-V3?\",\n",
    "    \"How does the hierarchical load balancing policy work in EPLB?\",\n",
    "    \"What are the benefits of using expert parallelism in large language models?\"\n",
    "]\n",
    "\n",
    "print(f\"Created test set with {len(test_questions)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Reference Answers\n",
    "\n",
    "To evaluate our models, we need reference answers. Since we don't have golden/human answers, we'll use some simple curated answers for key topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create some reference answers for a subset of questions\n",
    "# These will serve as rough \"ground truth\" for evaluation\n",
    "reference_answers = {\n",
    "    \"What is DualPipe and how does it improve training efficiency?\": \n",
    "        \"DualPipe is a bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It improves training efficiency by achieving full overlap of forward and backward computation-communication phases. DualPipe reduces pipeline bubbles by organizing computation and communication to happen simultaneously, effectively using previously idle GPU resources. It requires 2Ã— parameters but significantly improves throughput compared to traditional pipeline parallelism approaches like 1F1B and ZB1P.\",\n",
    "    \n",
    "    \"What is the Fire-Flyer File System (3FS) and what problems does it solve?\":\n",
    "        \"The Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to deliver high throughput. 3FS solves problems related to data access in AI workloads by providing a disaggregated architecture that combines throughput of thousands of SSDs, implementing strong consistency through Chain Replication with Apportioned Queries, and offering familiar file interfaces backed by transactional key-value stores. It efficiently handles diverse workloads including data preparation, dataloading, checkpointing, and KVCache for inference.\",\n",
    "    \n",
    "    \"What are the key innovations in DeepSeek-V3 that made it more efficient to train?\":\n",
    "        \"DeepSeek-V3 introduced several key innovations for training efficiency: (1) Mixture-of-Experts architecture with 671B parameters but only 37B active per token, (2) Multi-head Latent Attention (MLA) which compresses the Key-Value cache, (3) FP8 mixed precision training to reduce memory usage, (4) DualPipe bidirectional pipeline parallelism algorithm for efficient computation-communication overlap, (5) Expert Parallelism Load Balancing for optimized workload distribution, and (6) custom HAI-LLM training framework with efficient cross-node communication kernels. These innovations collectively made training significantly more efficient compared to traditional approaches.\"\n",
    "}\n",
    "\n",
    "print(f\"Created {len(reference_answers)} reference answers for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load evaluation metrics\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "# Run evaluations for all models on the test set\n",
    "eval_results = []\n",
    "\n",
    "for question in tqdm(test_questions, desc=\"Evaluating models\"):\n",
    "    try:\n",
    "        # Generate responses from all models\n",
    "        base_response = generate_base_response(question)\n",
    "        finetuned_response = generate_finetuned_response(question)\n",
    "        rag_response = generate_rag_response(question)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"base_response\": base_response,\n",
    "            \"finetuned_response\": finetuned_response,\n",
    "            \"rag_response\": rag_response\n",
    "        }\n",
    "        \n",
    "        # Calculate metrics if reference answer is available\n",
    "        if question in reference_answers:\n",
    "            reference = reference_answers[question]\n",
    "            result[\"reference\"] = reference\n",
    "            \n",
    "            # ROUGE scores\n",
    "            base_rouge = rouge.compute(predictions=[base_response], references=[reference])\n",
    "            ft_rouge = rouge.compute(predictions=[finetuned_response], references=[reference])\n",
    "            rag_rouge = rouge.compute(predictions=[rag_response], references=[reference])\n",
    "            \n",
    "            result[\"base_rouge_1\"] = base_rouge[\"rouge1\"]\n",
    "            result[\"base_rouge_2\"] = base_rouge[\"rouge2\"]\n",
    "            result[\"base_rouge_L\"] = base_rouge[\"rougeL\"]\n",
    "            \n",
    "            result[\"ft_rouge_1\"] = ft_rouge[\"rouge1\"]\n",
    "            result[\"ft_rouge_2\"] = ft_rouge[\"rouge2\"]\n",
    "            result[\"ft_rouge_L\"] = ft_rouge[\"rougeL\"]\n",
    "            \n",
    "            result[\"rag_rouge_1\"] = rag_rouge[\"rouge1\"]\n",
    "            result[\"rag_rouge_2\"] = rag_rouge[\"rouge2\"]\n",
    "            result[\"rag_rouge_L\"] = rag_rouge[\"rougeL\"]\n",
    "            \n",
    "            # BLEU scores (tokenize by splitting on whitespace)\n",
    "            base_bleu = bleu.compute(predictions=[base_response], references=[[reference]])\n",
    "            ft_bleu = bleu.compute(predictions=[finetuned_response], references=[[reference]])\n",
    "            rag_bleu = bleu.compute(predictions=[rag_response], references=[[reference]])\n",
    "            \n",
    "            result[\"base_bleu\"] = base_bleu[\"bleu\"]\n",
    "            result[\"ft_bleu\"] = ft_bleu[\"bleu\"]\n",
    "            result[\"rag_bleu\"] = rag_bleu[\"bleu\"]\n",
    "        \n",
    "        eval_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating question: {question}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame(eval_results)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(eval_dir / \"evaluation_results.csv\", index=False)\n",
    "print(f\"Saved evaluation results to {eval_dir / 'evaluation_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Aggregate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Filter to questions with reference answers for aggregate scores\n",
    "ref_results_df = results_df[results_df['question'].isin(reference_answers.keys())]\n",
    "\n",
    "# Calculate average scores\n",
    "metrics = [\"rouge_1\", \"rouge_2\", \"rouge_L\", \"bleu\"]\n",
    "model_types = [\"base\", \"ft\", \"rag\"]\n",
    "\n",
    "avg_scores = {}\n",
    "for model in model_types:\n",
    "    for metric in metrics:\n",
    "        column = f\"{model}_{metric}\"\n",
    "        if column in ref_results_df.columns:\n",
    "            avg_scores[column] = ref_results_df[column].mean()\n",
    "\n",
    "# Display average scores\n",
    "avg_df = pd.DataFrame([avg_scores])\n",
    "print(\"Average Scores across Reference Questions:\")\n",
    "display(avg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate response length statistics\n",
    "results_df['base_length'] = results_df['base_response'].apply(lambda x: len(x.split()))\n",
    "results_df['ft_length'] = results_df['finetuned_response'].apply(lambda x: len(x.split()))\n",
    "results_df['rag_length'] = results_df['rag_response'].apply(lambda x: len(x.split()))\n",
    "\n",
    "length_stats = {\n",
    "    'Base Model': [results_df['base_length'].mean(), results_df['base_length'].std()],\n",
    "    'Fine-tuned Model': [results_df['ft_length'].mean(), results_df['ft_length'].std()],\n",
    "    'RAG System': [results_df['rag_length'].mean(), results_df['rag_length'].std()]\n",
    "}\n",
    "\n",
    "length_df = pd.DataFrame(length_stats, index=['Mean Words', 'Std Dev'])\n",
    "print(\"Response Length Statistics:\")\n",
    "display(length_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot ROUGE-L scores for questions with reference answers\n",
    "plt.figure(figsize=(12, 6))\n",
    "ref_results = []\n",
    "for _, row in ref_results_df.iterrows():\n",
    "    ref_results.append({\n",
    "        'Question': row['question'],\n",
    "        'Base Model': row['base_rouge_L'],\n",
    "        'Fine-tuned Model': row['ft_rouge_L'],\n",
    "        'RAG System': row['rag_rouge_L']\n",
    "    })\n",
    "\n",
    "rouge_plot_df = pd.DataFrame(ref_results)\n",
    "rouge_plot_df = rouge_plot_df.melt(id_vars=['Question'], \n",
    "                                  var_name='Model Type', \n",
    "                                  value_name='ROUGE-L Score')\n",
    "\n",
    "sns.barplot(x='Question', y='ROUGE-L Score', hue='Model Type', data=rouge_plot_df)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.title('ROUGE-L Scores by Question and Model Type')\n",
    "plt.savefig(eval_dir / \"rouge_scores.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot aggregate comparison of ROUGE and BLEU scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "metrics_to_plot = ['rouge_1', 'rouge_2', 'rouge_L', 'bleu']\n",
    "metrics_labels = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU']\n",
    "model_colors = {'base': 'blue', 'ft': 'green', 'rag': 'red'}\n",
    "model_labels = {'base': 'Base Model', 'ft': 'Fine-tuned Model', 'rag': 'RAG System'}\n",
    "\n",
    "x = np.arange(len(metrics_labels))\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(model_types):\n",
    "    values = [avg_scores.get(f\"{model}_{metric}\", 0) for metric in metrics_to_plot]\n",
    "    plt.bar(x + (i - 1) * width, values, width, label=model_labels[model], color=model_colors[model])\n",
    "\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Comparison of Models Across Metrics')\n",
    "plt.xticks(x, metrics_labels)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(eval_dir / \"model_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot response length distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "length_data = [\n",
    "    results_df['base_length'].tolist(),\n",
    "    results_df['ft_length'].tolist(),\n",
    "    results_df['rag_length'].tolist()\n",
    "]\n",
    "\n",
    "plt.violinplot(length_data, showmeans=True)\n",
    "plt.boxplot(length_data, positions=[1, 2, 3], widths=0.15, patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightblue'), showfliers=False)\n",
    "plt.xticks([1, 2, 3], ['Base Model', 'Fine-tuned Model', 'RAG System'])\n",
    "plt.ylabel('Response Length (words)')\n",
    "plt.title('Distribution of Response Lengths')\n",
    "plt.savefig(eval_dir / \"response_lengths.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select a few representative questions for qualitative analysis\n",
    "qualitative_questions = [\n",
    "    \"What is DualPipe and how does it improve training efficiency?\",\n",
    "    \"How does the DeepSeek-V3 model use Mixture-of-Experts architecture?\",\n",
    "    \"What are the advantages of the Chain Replication with Apportioned Queries (CRAQ) in 3FS?\"\n",
    "]\n",
    "\n",
    "# Display responses from all models for these questions\n",
    "for question in qualitative_questions:\n",
    "    result = results_df[results_df['question'] == question].iloc[0]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"Base Model Response:\")\n",
    "    print(\"-\"*40)\n",
    "    print(result['base_response'][:500], \"...\\n(truncated)\" if len(result['base_response']) > 500 else \"\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Fine-tuned Model Response:\")\n",
    "    print(\"-\"*40)\n",
    "    print(result['finetuned_response'][:500], \"...\\n(truncated)\" if len(result['finetuned_response']) > 500 else \"\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"RAG System Response:\")\n",
    "    print(\"-\"*40)\n",
    "    print(result['rag_response'][:500], \"...\\n(truncated)\" if len(result['rag_response']) > 500 else \"\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    if 'reference' in result and not pd.isna(result['reference']):\n",
    "        print(\"Reference Answer:\")\n",
    "        print(\"-\"*40)\n",
    "        print(result['reference'])\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to identify major errors or hallucinations in responses\n",
    "def analyze_errors(response):\n",
    "    # List of technical terms that should be correctly used\n",
    "    technical_terms = [\n",
    "        \"DualPipe\", \"3FS\", \"Fire-Flyer\", \"CRAQ\", \"Mixture-of-Experts\", \"MoE\", \n",
    "        \"EPLB\", \"Multi-head Latent Attention\", \"MLA\"\n",
    "    ]\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    # Check for \"I don't know\" or uncertainty indicators\n",
    "    uncertainty_phrases = [\"I don't know\", \"I'm not sure\", \"I don't have information\", \n",
    "                          \"I cannot provide\", \"I'm not familiar\"]\n",
    "    for phrase in uncertainty_phrases:\n",
    "        if phrase.lower() in response.lower():\n",
    "            errors.append(f\"Uncertainty detected: '{phrase}'\")\n",
    "    \n",
    "    # Check for inconsistent technical term usage\n",
    "    for term in technical_terms:\n",
    "