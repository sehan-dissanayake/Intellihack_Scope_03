# 202502 Open-Source Week

We're a tiny team @deepseek-ai pushing our limits in AGI exploration.

Starting this week , Feb 24, 2025 we'll open-source 5 repos â€“ one daily drop â€“ not because we've made grand claims, but simply as developers sharing our small-but-sincere progress with full transparency.

These are humble building blocks of our online service: documented, deployed and battle-tested in production. No vaporware, just sincere code that moved our tiny yet ambitious dream forward.

Why? Because every line shared becomes collective momentum that accelerates the journey. Daily unlocks begin soon. No ivory towers - just pure garage-energy and community-driven innovation ğŸ”§

Stay tuned â€“ let's geek out in the open together.

## Day 1 - FlashMLA

Efficient MLA Decoding Kernel for Hopper GPUs
Optimized for variable-length sequences, battle-tested in production

ğŸ”— FlashMLA GitHub Repo
âœ… BF16 support
âœ… Paged KV cache (block size 64)
âš¡ Performance: 3000 GB/s memory-bound | BF16 580 TFLOPS compute-bound on H800

## Day 2 - DeepEP

Excited to introduce DeepEP - the first open-source EP communication library for MoE model training and inference.

ğŸ”— DeepEP GitHub Repo
âœ… Efficient and optimized all-to-all communication
âœ… Both intranode and internode support with NVLink and RDMA
âœ… High-throughput kernels for training and inference prefilling
âœ… Low-latency kernels for inference decoding
âœ… Native FP8 dispatch support
âœ… Flexible GPU resource control for computation-communication overlapping

## Day 3 - DeepGEMM

Introducing DeepGEMM - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3/R1 training and inference.

ğŸ”— DeepGEMM GitHub Repo
âš¡ Up to 1350+ FP8 TFLOPS on Hopper GPUs
âœ… No heavy dependency, as clean as a tutorial
âœ… Fully Just-In-Time compiled
âœ… Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes
âœ… Supports dense layout and two MoE layouts

## Day 4 - Optimized Parallelism Strategies

âœ… DualPipe - a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.
ğŸ”— GitHub Repo

âœ… EPLB - an expert-parallel load balancer for V3/R1.
ğŸ”— GitHub Repo

ğŸ“Š Analyze computation-communication overlap in V3/R1.
ğŸ”— GitHub Repo

## Day 5 - 3FS, Thruster for All DeepSeek Data Access

Fire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.

âš¡ 6.6 TiB/s aggregate read throughput in a 180-node cluster
âš¡ 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster
âš¡ 40+ GiB/s peak throughput per client node for KVCache lookup
ğŸ§¬ Disaggregated architecture with strong consistency semantics
âœ… Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search & KVCache lookups for inference in V3/R1

ğŸ“¥ 3FS â†’ https://github.com/deepseek-ai/3FS
â›² Smallpond - data processing framework on 3FS â†’ https://github.com/deepseek-ai/smallpond

## Day 6 - One More Thing: DeepSeek-V3/R1 Inference System Overview

Optimized throughput and latency via:
ğŸ”§ Cross-node EP-powered batch scaling
ğŸ”„ Computation-communication overlap
âš–ï¸ Load balancing

Production data of V3/R1 online services:
âš¡ 73.7k/14.8k input/output tokens per second per H800 node
ğŸš€ Cost profit margin 545%